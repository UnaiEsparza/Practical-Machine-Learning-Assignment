---
title: 'Practical Machine Learning Peer-graded Assignment: Prediction Assignment Writeup'
author: "U.Esparza"
date: "8/12/2020"
output: html_document
---
### Synopsis

###### The aim of this study is to predict the manner ("classe") in which some healthy subjects performed a weight lifting exercise. 

###### The subjects carried out the excercise in different fashions (some correct and some wrong). Their movements were monitorized using devices equipped with accelerometers and stored in datasets that are available in the "WayBack Machine" website: http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har.

### Data download and required package loading

```{r, echo=TRUE}

library(AppliedPredictiveModeling)
library(caret)
library(dplyr)
library(randomForest)

urlTrain <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
urlTest <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

download.file(urlTrain, destfile =  "./pml-training.csv")
download.file(urlTest, destfile =  "./pml-testing.csv")

training <- read.csv("pml-training.csv", na.strings=c("", "NA"))
testing <- read.csv("pml-testing.csv", na.strings=c("", "NA"))
unique(training$classe)

```

### Data Exploratory Analysis

###### The str() and table() functions are used to understand the basic structure of the dataset. Due to the high number of columns (160), the result is subsetted:

```{r, echo=TRUE}

ncol(training)
str(training[,1:10]) # fist 10 columns. The first variables are not actual predictors
str(training[,149:160]) # last 12 columns. The outcome Classe appears at the end. Some columns appear to have plenty of NAs. 

table(training$classe,training$user_name) # number of observations per "user_name" and per "classe"

ggplot(training, aes(classe)) + geom_bar(fill = "steelblue") + ggtitle("Counts per classe")


```

###### The plot shows that there is a relatively balanced distribution of observations among "classe" types.

### Data pre-processing

###### The outcome "classe" must be converted into a factor variable. Additionally, there are many columns which do not provide any relevant information, because they either have plenty of NAs or because they are not actual predictors obtained from accelerator measurements. Those columns will be removed:

```{r, echo=TRUE}

training$classe <- as.factor(training$classe) # classe is converted into a factor variable.

trainingPrep <- training %>% select(8:160) # Non-predictors are removed.

trainingPrep <- trainingPrep %>% select_if(colSums(is.na(trainingPrep)) < 19000) # Only the columns with LESS than 19000 NAs are left (total nr. of obs. is 19622)

ncol(trainingPrep) # The resulting amount of columns in the dataset is 53.

```

### Create Data Partition

###### This dataset is further divided into train (75%) and test (25%) parts for cross-validation:

```{r, echo=TRUE}

inTrain = createDataPartition(trainingPrep$classe, p = 3/4)[[1]]
trainPart = trainingPrep[ inTrain,]
testPart = trainingPrep[-inTrain,]

```

### Model training

###### A couple of models will be trained and tested with cross validation to find out which of them has the highest accuracy level. More precisely, a random forest model and an LDA model will be tested:

```{r, echo=TRUE}

set.seed(1234)
modfitrf <- randomForest(classe~., method = "class", data = trainPart)
predrf <- predict(modfitrf, newdata = testPart, type = "class")
confusionMatrix(predrf, testPart$classe)

set.seed(1234)
modfitlda <- train(classe ~ ., method = "lda", data = trainPart)
predlda <- predict(modfitlda, newdata = testPart)
confusionMatrix(predlda, testPart$classe)

```


### Model selection

###### The accuracy level of the random forest model (higher than 99%) is clearly higher than that of the LDA model (close to 70%). Therefore, the random forest model is selected.

### Cross validation and expected out of sample error

###### The out of sample error (calculated as 1 - Accuracy Level) is below 1%, therefore very low.


### Prediction on 20 test cases

```{r, echo=TRUE}

predrf20 <- predict(modfitrf, newdata = testing, type = "class")
print(predrf20)

```

###### This is the prediction achieved with the selected model (random forest) for the 20 test cases.

